# Configuration for comparison phase only
output:
  output_dir: "outputs/comparison"

ranking:
  # Metrics to collect/rank. If empty, falls back to clearml.comparison_metrics (or defaults).
  metrics: ["r2", "rmse", "mae"]
  # Ranking key. Use "composite_score" to rank by weighted composite score.
  primary_metric: "r2"
  # Optional override ("min" or "max"). If null, auto-detect by metric name.
  goal: null
  # Limit plots/tables in ClearML (null = no limit)
  top_k: 50
  composite:
    enabled: true
    # Metrics used for composite_score (empty = use ranking.metrics / weights keys)
    metrics: ["r2", "rmse", "mae"]
    # Weight per metric (higher = more important). Missing metrics default to 1.0.
    weights:
      r2: 1.0
      rmse: 1.0
      mae: 0.5
    # If true, rows missing any composite metric become NaN for composite_score.
    require_all_metrics: false

clearml:
  enabled: true
  project_name: "AutoML-with-ClearML"
  queue: "services"
  tags: ["automl", "comparison"]
  comparison_task_name: "comparison"
  comparison_metrics: ["r2", "rmse", "mae"]
